import streamlit as st
import psycopg
import os
from groq import Groq
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv


# Configuration de la page
st.set_page_config(
    page_title="Chatbot",
    page_icon="üí¨",
    layout="centered"
)

# Param√®tres de connexion
DB_NAME = "rag_chatbot"
DB_USER = "postgres"
DB_PASSWORD = "1234"
DB_HOST = "localhost"
DB_PORT = "5433"
load_dotenv()
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
db_connection_str = f"dbname={DB_NAME} user={DB_USER} password={DB_PASSWORD} host={DB_HOST} port={DB_PORT}"

# Initialisation des mod√®les (mis en cache)
@st.cache_resource
def load_embedding_model():
    return SentenceTransformer('all-MiniLM-L6-v2')

@st.cache_resource
def initialize_groq_client():
    return Groq(api_key=GROQ_API_KEY)

embedding_model = load_embedding_model()
groq_client = initialize_groq_client()


def calculate_embeddings(corpus: str) -> list[float]:
    """G√©n√©rer des embeddings pour un texte."""
    embedding = embedding_model.encode(corpus, convert_to_numpy=True)
    return embedding.tolist()


def similar_corpus(input_corpus: str, top_k: int = 3) -> list[tuple[int, str, float]]:
    """Trouver les entr√©es similaires dans la base de donn√©es."""
    try:
        input_embedding = calculate_embeddings(input_corpus)
        
        with psycopg.connect(db_connection_str) as conn:
            with conn.cursor() as cur:
                cur.execute(
                    """
                    SELECT id, corpus, embedding <=> %s::vector AS distance
                    FROM embeddings
                    ORDER BY distance
                    LIMIT %s
                    """,
                    (input_embedding, top_k)
                )
                results = cur.fetchall()
                return results
    except Exception as e:
        print(f"Erreur de connexion DB : {e}")
        return []


def query_with_context(question: str) -> tuple[str, list]:
    """Interroger le chatbot avec RAG. Retourne (r√©ponse, sources)."""
    # Trouver les entr√©es similaires
    similar_entries = similar_corpus(question, top_k=3)
    
    if not similar_entries:
        return "D√©sol√©, je n'ai pas pu trouver d'informations pertinentes.", []
    
    # Construire le contexte
    context = "\n".join([entry[1] for entry in similar_entries])
    
    # Cr√©er le prompt
    prompt = f"""En te basant sur le contexte de conversation suivant, r√©ponds √† la question en fran√ßais de mani√®re claire et pr√©cise.

Contexte :
{context}

Question : {question}

R√©ponse :"""
    
    # G√©n√©rer la r√©ponse
    try:
        chat_completion = groq_client.chat.completions.create(
            messages=[{"role": "user", "content": prompt}],
            model="llama-3.3-70b-versatile",
            temperature=0.7,
            max_tokens=1024,
        )
        response = chat_completion.choices[0].message.content
        return response, similar_entries
    except Exception as e:
        print(f"Erreur d√©taill√©e : {e}")
        error_msg = f"Erreur lors de la g√©n√©ration de la r√©ponse. V√©rifiez votre cl√© API Groq."
        return error_msg, []


# CSS personnalis√© pour un style √©pur√©
st.markdown("""
    <style>
    .main {
        max-width: 800px;
        margin: 0 auto;
    }
    .stChatMessage {
        padding: 1rem;
        border-radius: 10px;
    }
    </style>
    """, unsafe_allow_html=True)

# Titre simple
st.title("üí¨ Vous avez une question?")

# Initialiser l'historique des messages
if "messages" not in st.session_state:
    st.session_state.messages = []

# Afficher l'historique des messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        
        # Afficher les r√©f√©rences pour l'assistant
        if message["role"] == "assistant" and "sources" in message and message["sources"]:
            with st.expander("üìö Voir les sources", expanded=False):
                for i, (id, corpus, distance) in enumerate(message["sources"], 1):
                    similarity = (1 - distance) * 100
                    st.markdown(f"**Source {i}** ‚Ä¢ Pertinence: {similarity:.1f}%")
                    st.caption(corpus[:300] + "..." if len(corpus) > 300 else corpus)
                    if i < len(message["sources"]):
                        st.divider()

# Input utilisateur
if prompt := st.chat_input("√âcrivez votre message..."):
    # Ajouter et afficher le message de l'utilisateur
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # G√©n√©rer et afficher la r√©ponse
    with st.chat_message("assistant"):
        with st.spinner(""):
            response, sources = query_with_context(prompt)
        st.markdown(response)
        
        # Afficher les sources/r√©f√©rences
        if sources:
            with st.expander("üìö Voir les sources", expanded=False):
                for i, (id, corpus, distance) in enumerate(sources, 1):
                    similarity = (1 - distance) * 100
                    st.markdown(f"**Source {i}** ‚Ä¢ Pertinence: {similarity:.1f}%")
                    st.caption(corpus[:300] + "..." if len(corpus) > 300 else corpus)
                    if i < len(sources):
                        st.divider()
    
    # Ajouter la r√©ponse √† l'historique
    st.session_state.messages.append({
        "role": "assistant", 
        "content": response,
        "sources": sources
    })